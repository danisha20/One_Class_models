{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of OCC DNN.ipynb","provenance":[{"file_id":"1VspFcV4ZXuoaLtskgcp9aR5nO4bFGfGj","timestamp":1602800879212}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"AdA-1G9Tcgn5"},"source":["# Basic libraries\n","import numpy as np \n","import pandas as pd\n","import os\n","# Preprocessing libraries\n","from sklearn.model_selection import train_test_split \n","from sklearn import metrics\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","# Anomaly detection algorithms\n","from sklearn.ensemble import IsolationForest\n","from pyod.models.lmdd import LMDD\n","from pyod.models.copod import COPOD\n","from pyod.models.feature_bagging import FeatureBagging\n","from sklearn.covariance import EllipticEnvelope\n","from pyod.models.knn import KNN\n","from pyod.models.loda import LODA\n","from pyod.models.sos import SOS\n","from pyod.models.mcd import MCD\n","from sklearn.neighbors import LocalOutlierFactor\n","from sklearn.mixture import GaussianMixture\n","from pyod.models.cblof import CBLOF\n","from pyod.models.hbos import HBOS\n","from pyod.models.sod import SOD\n","from pyod.models.xgbod import XGBOD\n","from pyod.models.pca import PCA\n","from pyod.models.loci import LOCI\n","# Neural network based outlier detection\n","from pyod.models.vae import VAE\n","from pyod.models.auto_encoder import AutoEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AnomalyTester():\n","\n","    def __init__(self, model, model_name, rootDir, scaler=None):\n","        self.model = model\n","        self.model_name = model_name\n","        self.scaler = scaler\n","        self.rootDir = rootDir\n","\n","        self.run_all_files()\n","\n","    def importdata(self, trainfile_path, testfile_path):\n","        '''\n","        Imports the data of the train and test files\n","        '''\n","        self.df_train = pd.read_csv(trainfile_path, sep= ',') \n","        self.df_test = pd.read_csv(testfile_path, sep= ',') \n","        \n","        return self.df_train, self.df_test\n","\n","    # Function to split target from data \n","    def splitdataset(self, train, test):\n","        '''\n","        It splits the dataset\n","        '''\n","        # Call the OneHotEncoder class\n","        ohe = OneHotEncoder(sparse=True)\n","        # Concatenate all data \n","        allData = pd.concat([train, test], ignore_index=True, sort =False, axis=0)\n","        # Omit the response variable (the last column)\n","        AllDataWihoutClass = allData.iloc[:, :-1]\n","        # Select only nominal types\n","        AllDataWihoutClassOnlyNominals = AllDataWihoutClass.select_dtypes(include=['object'])\n","        # Select numerical types\n","        AllDataWihoutClassNoNominals = AllDataWihoutClass.select_dtypes(exclude=['object'])\n","        # one hot encoding of all nominals\n","        encAllDataWihoutClassNominals = ohe.fit_transform(AllDataWihoutClassOnlyNominals)\n","        # get data without class to a dataframe\n","        encAllDataWihoutClassNominalsToPanda = pd.DataFrame(encAllDataWihoutClassNominals.toarray())\n","        encAllDataWihoutClassNominalsToPanda = encAllDataWihoutClassNominalsToPanda.astype(object)\n","        # If the dataset contain more tha 0 columns of nominals then concatenate if not pass them      \n","        if AllDataWihoutClassOnlyNominals.shape[1] > 0:\n","            codAllDataAgain = pd.concat([encAllDataWihoutClassNominalsToPanda,\n","                                AllDataWihoutClassNoNominals], ignore_index=True, sort =False, axis=1)\n","        else:\n","            codAllDataAgain = AllDataWihoutClass\n","        # Seperating the target variable \n","        self.X = codAllDataAgain[:len(allData)]\n","        self.y_real = allData.values[:, -1]\n","        self.y_train = ['negative' for _ in range(len(allData))]\n","\n","        return self.X, self.y_real, self.y_train\n","    \n","    def scaler_transform(self, X):\n","        '''\n","        Transform the NON-object type data to the selected scaler\n","        '''\n","        X_withoutobj = X.select_dtypes(exclude=['object'])\n","        # Fit transform the scaler if there are objects in dataset\n","        if X_withoutobj.shape[1] > 0:\n","            X_withoutobj = pd.DataFrame(self.scaler.fit_transform(X_withoutobj[X_withoutobj.columns]),\n","                                            index=X_withoutobj.index,\n","                                            columns=X_withoutobj.columns)\n","            # Concatenate the standard\n","            idx = X.columns.get_indexer(X.select_dtypes('object').columns)\n","            for i in range(len(idx)):\n","                X_withoutobj.insert(i, i, X[[i]], True)\n","            self.X = X_withoutobj\n","        else:\n","            self.X = X\n","\n","        return self.X\n","\n","    def get_score(self, X, clf_object):\n","        '''\n","        Gets the score\n","        '''\n","        try:\n","            self.y_score = clf_object.score_samples(X) \n","        except AttributeError:\n","            try:\n","                self.y_score = clf_object.decision_function(X)\n","            except:\n","                self.y_score = clf_object.decision_function(X.values)\n","    \n","        return self.y_score\n","    \n","    def run_all_files(self):\n","        '''\n","        Iterates through all files in a root directory, trains and evaluates all data.\n","        '''\n","        print('Starting '+self.model_name)\n","        data = {'folder_name':[], self.model_name+'_auc':[], self.model_name+'_avgprecision':[]}\n","        for dirName, subdirList, fileList in os.walk(self.rootDir):\n","            if len(fileList) > 0:\n","                arr_auc = []\n","                arr_ave_precision = []\n","                arr_folder_name = dirName.split(\"/\")\n","                folder_name = arr_folder_name[len(arr_folder_name) - 1]\n","                completed_name = folder_name + \"-5-\"\n","                for i in range(1, int(len(fileList) / 2) + 1):\n","                    #print('Dataset in process...') \n","                    trainFile = str(dirName) + '/' + completed_name + str(i) +\"tra.csv\"\n","                    testFile = str(dirName) + '/' + completed_name + str(i) +\"tst.csv\"\n","                    print('Model: '+self.model_name+' Train File: ' + completed_name + str(i))\n","                    # Loading the data\n","                    df_train, df_test = self.importdata(trainFile, testFile)\n","                    # Split the dataset\n","                    X, y_real, y_train = self.splitdataset(df_train, df_test)\n","                    # Scale the data if indicated\n","                    if self.scaler != None:\n","                        X = self.scaler_transform(X)\n","                    # Fit the model (some linear models require only float or int)\n","                    if self.model_name == 'AAD_LMDD' or self.model_name == 'IQR_LMDD' or self.model_name == 'AAD_LMDD_std' or self.model_name == 'AAD_LMDD_mm' or self.model_name == 'IQR_LMDD_std' or self.model_name == 'IQR_LMDD_mm':\n","                        X = X.astype(float)\n","                    self.model.fit(X, y_train) # some algorithms require y_train, but it is all neg.\n","                    # Predict\n","                    y_score_classif = self.get_score(X, self.model) \n","                    # Get the evaluation\n","                    auc = metrics.roc_auc_score(y_real,  y_score_classif) # y originals\n","                    ave_precision = metrics.average_precision_score(y_real, # y originals\n","                                                                y_score_classif,\n","                                                                pos_label='positive')\n","                    # Append the results\n","                    arr_auc.append(1 - auc if auc < 0.5 else auc)\n","                    arr_ave_precision.append(1-ave_precision if ave_precision<0.5 else ave_precision)\n","                # Calculate the average of scores\n","                result_auc = sum(arr_auc) / len(arr_auc)\n","                result_ave_precision = sum(arr_ave_precision) / len(arr_ave_precision)\n","                # Save them to the resulting dataframe\n","                data['folder_name'].append(folder_name)\n","                data[self.model_name+'_auc'].append(result_auc)\n","                data[self.model_name+'_avgprecision'].append(result_ave_precision)\n","            else:\n","                pass\n","            # Save it in a dataframe\n","            df = pd.DataFrame(data, columns = ['folder_name', self.model_name+'_auc', self.model_name+'_avgprecision'])\n","            # Export the document to csv\n","            df.to_csv(self.model_name+'_results.csv', index=False)\n","        print('\\nFinished '+self.model_name)\n","\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["# Specify the root directory\n","rootDir = 'G:/My Drive/Github/ml-group-col/One-Class-models/Anomaly_Datasets_csv/'\n","# specify the random state\n","rs = 10\n","# Save how to run the models\n","models = [(IsolationForest(random_state=rs),'ISOF'),\n","            (EllipticEnvelope(random_state=rs),'EE'),\n","            (LMDD(dis_measure='aad', random_state=rs),'AAD_LMDD'),\n","            (COPOD(),'COPOD'),\n","            (FeatureBagging(combination='average', random_state=rs),'AVE_Bagging'), # n_jobs\n","            (LMDD(dis_measure='iqr',random_state=rs),'IQR_LMDD'),\n","            (KNN(method='largest'),'Largest_KNN'), # n_jobs\n","            (LODA(),'LODA'),\n","            (FeatureBagging(combination='max', n_jobs=-1, random_state=rs),'MAX_Bagging'),\n","            (MCD(random_state=rs),'MCD'),\n","            (XGBOD(random_state=rs),'XGBOD'), # n_jobs\n","            (GaussianMixture(random_state=rs),'GMM'),\n","            (LocalOutlierFactor(novelty=True),'LOF'),\n","            (KNN(method='median'),'Median_KNN'), # n_jobs\n","            (KNN(method='mean'),'Avg_KNN'), # n_jobs\n","            (CBLOF(n_clusters=10,random_state=rs),'CBLOF'),\n","            (HBOS(),'HBOS'), \n","            (SOD(), 'SOD'),\n","            (PCA(random_state=rs),'PCA'),\n","            (VAE(encoder_neurons=[3,4,3], decoder_neurons=[3,4,3],random_state=rs),'VAE'),\n","            (AutoEncoder(hidden_neurons=[3, 4, 4, 3], verbose=0,random_state=rs),'AE')]\n","# Select the model location with i to run\n","i = 0\n","# Initialize the class anomaly\n","AnomalyTester(models[i][0],models[i][1], rootDir)\n","AnomalyTester(models[i][0],models[i][1]+'_std', rootDir, StandardScaler())\n","AnomalyTester(models[i][0],models[i][1]+'_mm', rootDir, MinMaxScaler())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}